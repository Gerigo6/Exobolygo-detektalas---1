{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds5yXAU8dp8K"
      },
      "source": [
        "##Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6fbxKfyzQIj"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy tensorflow scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VBoV_wPzSyB"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0gy5lb-zUZy"
      },
      "outputs": [],
      "source": [
        "pip install -U imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYEHyhe7zXIj"
      },
      "outputs": [],
      "source": [
        "pip install lightkurve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nN0zkAYdpfC"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import lightkurve as lk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import concurrent.futures\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, concatenate, BatchNormalization, Activation, LeakyReLU, Multiply, Permute, Reshape, Lambda, RepeatVector\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, make_scorer, roc_auc_score, precision_recall_curve\n",
        "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import ADASYN, BorderlineSMOTE, SMOTE\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyzuUs7gdtnS"
      },
      "source": [
        "## Adatok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Liang Yu és munkatársai által közzétett adatok tensorflow record formátumúak, így először egy parszer segítségével átkonvertálom őket pandas dataframmé."
      ],
      "metadata": {
        "id": "VE18Yj-FKqRQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qCQ9b5gdj1M"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example):\n",
        "    features = {\n",
        "        'tic_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'row_id': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'Epoc': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'Sectors': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'local_view': tf.io.FixedLenFeature([61], tf.float32),\n",
        "        'secondary_view': tf.io.FixedLenFeature([61], tf.float32),\n",
        "        'Transit_Depth': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'global_view': tf.io.FixedLenFeature([201], tf.float32),\n",
        "        'Duration': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'ccd': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'Period': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'depth_change': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'camera': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'Disposition': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "\n",
        "    return tf.io.parse_single_example(example, features)\n",
        "\n",
        "def read_dataset(file_pattern):\n",
        "    files = tf.io.gfile.glob(file_pattern)\n",
        "    dataset = tf.data.TFRecordDataset(files)\n",
        "    return dataset.map(parse_tfrecord)\n",
        "\n",
        "def dataset_to_dataframe(dataset):\n",
        "    records = []\n",
        "    for record in dataset:\n",
        "        parsed_record = {key: value.numpy() for key, value in record.items()}\n",
        "        records.append(parsed_record)\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "train_pattern = ['train-0000{}-of-00008'.format(i) for i in range(8)]\n",
        "test_pattern = 'test-00000-of-00001'\n",
        "val_pattern = 'val-00000-of-00001'\n",
        "\n",
        "train_dataset = read_dataset(train_pattern)\n",
        "test_dataset = read_dataset(test_pattern)\n",
        "val_dataset = read_dataset(val_pattern)\n",
        "\n",
        "train_df = dataset_to_dataframe(train_dataset)\n",
        "test_df = dataset_to_dataframe(test_dataset)\n",
        "val_df = dataset_to_dataframe(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "0BkoZu_fqi74",
        "outputId": "eb134a89-ea85-4060-c2ba-7ec8b4df447b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Disposition  Duration         Epoc    Period  Sectors  Transit_Depth  \\\n",
              "0            b'J'  0.018976  1325.519165  0.117514        2  -51882.871094   \n",
              "1            b'J'  0.156284  1357.567749  1.378893        2  -51882.871094   \n",
              "2            b'J'  0.363797  1354.612671  0.662148        2  -51882.871094   \n",
              "3            b'J'  0.272520  1325.816895  0.739860        2  -51882.871094   \n",
              "4            b'J'  0.034525  1354.342773  0.168363        2  -51882.875000   \n",
              "...           ...       ...          ...       ...      ...            ...   \n",
              "13145        b'J'  0.306524  1356.508667  2.431377        2  -51882.875000   \n",
              "13146        b'J'  0.018647  1325.826538  0.157572        4  -51882.871094   \n",
              "13147        b'J'  0.086853  1326.775513  0.652589        4  -51882.875000   \n",
              "13148       b'PC'  0.162419  1326.658447  2.166031        1  -51882.871094   \n",
              "13149        b'J'  0.121973  1332.107910  5.584831        4  -51882.875000   \n",
              "\n",
              "       camera  ccd  depth_change  \\\n",
              "0           2    1      0.021858   \n",
              "1           1    2      0.514410   \n",
              "2           1    1      0.531969   \n",
              "3           4    2      0.064786   \n",
              "4           4    2      0.037692   \n",
              "...       ...  ...           ...   \n",
              "13145       3    1     -0.076715   \n",
              "13146       4    3      0.029525   \n",
              "13147       4    3      0.020843   \n",
              "13148       4    3      0.016130   \n",
              "13149       4    3      0.051618   \n",
              "\n",
              "                                             global_view  \\\n",
              "0      [-0.26562434, -0.61626935, -0.5845301, -0.5046...   \n",
              "1      [1.4579551, 1.581101, 1.124804, 1.2362709, 1.0...   \n",
              "2      [0.43146962, 0.23385176, 0.6106999, 0.60349447...   \n",
              "3      [0.23947957, 0.09840288, 0.07395018, 0.4272265...   \n",
              "4      [-0.7914866, -0.56841093, -0.90398467, -0.4970...   \n",
              "...                                                  ...   \n",
              "13145  [0.25040475, 0.1748516, -0.25876957, 0.4646519...   \n",
              "13146  [-0.8488945, -0.76858133, -0.59189904, -0.4129...   \n",
              "13147  [0.123891085, -0.055945847, 0.07097072, -0.223...   \n",
              "13148  [-0.086693116, -0.07344482, -0.07668034, -0.09...   \n",
              "13149  [0.7601237, 0.7964004, 0.663667, 0.685883, 0.7...   \n",
              "\n",
              "                                              local_view  row_id  \\\n",
              "0      [0.15251282, 0.34120828, 0.49230614, 0.5015539...    3594   \n",
              "1      [0.8999962, 0.6396017, 0.61073464, -0.01752643...    5278   \n",
              "2      [0.96172047, 1.0056368, 0.8330003, 0.8792302, ...    5322   \n",
              "3      [0.28603047, 0.07896457, 0.20502119, 0.1732962...    6729   \n",
              "4      [-0.36739764, -0.2959777, -0.2437606, -0.16561...    1959   \n",
              "...                                                  ...     ...   \n",
              "13145  [0.5349816, 0.10783798, 0.0, 0.062072594, 0.20...    1201   \n",
              "13146  [1.103685, 0.92982835, 0.54336816, 0.52445084,...   11523   \n",
              "13147  [0.6529525, 0.69058144, 0.80414826, 0.58449507...    8400   \n",
              "13148  [0.20690493, 0.08123834, -0.010720634, 0.09892...    5836   \n",
              "13149  [0.022403259, -0.06456212, -0.06761711, 0.3498...   11604   \n",
              "\n",
              "                                          secondary_view     tic_id  \n",
              "0      [-0.6317023, -0.2648961, 0.014789562, -0.08227...  201291168  \n",
              "1      [-0.10018177, 0.028247697, 0.0, 0.16691712, 0....   70775438  \n",
              "2      [-0.8480952, -0.6290109, -0.5988463, -0.442735...   76944565  \n",
              "3      [-0.32621714, -0.66402626, -0.77617854, -0.048...  179444779  \n",
              "4      [-0.19288762, -0.09012802, -0.005803698, -0.00...   30631031  \n",
              "...                                                  ...        ...  \n",
              "13145  [-0.5099395, -0.2610199, -0.013828868, 0.03111...  160196644  \n",
              "13146  [-0.1820444, -0.18216795, -0.339928, -0.344022...  270471974  \n",
              "13147  [-1.0, -0.57836837, -0.66402966, -0.7568603, -...  141187719  \n",
              "13148  [-0.07755127, -0.2760155, 0.1085413, 0.6567023...  349518800  \n",
              "13149  [-0.47620365, 0.05174322, -0.7024073, -0.64429...  271693788  \n",
              "\n",
              "[13150 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed103d75-5094-44a5-be2d-4ecb328220ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Disposition</th>\n",
              "      <th>Duration</th>\n",
              "      <th>Epoc</th>\n",
              "      <th>Period</th>\n",
              "      <th>Sectors</th>\n",
              "      <th>Transit_Depth</th>\n",
              "      <th>camera</th>\n",
              "      <th>ccd</th>\n",
              "      <th>depth_change</th>\n",
              "      <th>global_view</th>\n",
              "      <th>local_view</th>\n",
              "      <th>row_id</th>\n",
              "      <th>secondary_view</th>\n",
              "      <th>tic_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.018976</td>\n",
              "      <td>1325.519165</td>\n",
              "      <td>0.117514</td>\n",
              "      <td>2</td>\n",
              "      <td>-51882.871094</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0.021858</td>\n",
              "      <td>[-0.26562434, -0.61626935, -0.5845301, -0.5046...</td>\n",
              "      <td>[0.15251282, 0.34120828, 0.49230614, 0.5015539...</td>\n",
              "      <td>3594</td>\n",
              "      <td>[-0.6317023, -0.2648961, 0.014789562, -0.08227...</td>\n",
              "      <td>201291168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.156284</td>\n",
              "      <td>1357.567749</td>\n",
              "      <td>1.378893</td>\n",
              "      <td>2</td>\n",
              "      <td>-51882.871094</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.514410</td>\n",
              "      <td>[1.4579551, 1.581101, 1.124804, 1.2362709, 1.0...</td>\n",
              "      <td>[0.8999962, 0.6396017, 0.61073464, -0.01752643...</td>\n",
              "      <td>5278</td>\n",
              "      <td>[-0.10018177, 0.028247697, 0.0, 0.16691712, 0....</td>\n",
              "      <td>70775438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.363797</td>\n",
              "      <td>1354.612671</td>\n",
              "      <td>0.662148</td>\n",
              "      <td>2</td>\n",
              "      <td>-51882.871094</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.531969</td>\n",
              "      <td>[0.43146962, 0.23385176, 0.6106999, 0.60349447...</td>\n",
              "      <td>[0.96172047, 1.0056368, 0.8330003, 0.8792302, ...</td>\n",
              "      <td>5322</td>\n",
              "      <td>[-0.8480952, -0.6290109, -0.5988463, -0.442735...</td>\n",
              "      <td>76944565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.272520</td>\n",
              "      <td>1325.816895</td>\n",
              "      <td>0.739860</td>\n",
              "      <td>2</td>\n",
              "      <td>-51882.871094</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.064786</td>\n",
              "      <td>[0.23947957, 0.09840288, 0.07395018, 0.4272265...</td>\n",
              "      <td>[0.28603047, 0.07896457, 0.20502119, 0.1732962...</td>\n",
              "      <td>6729</td>\n",
              "      <td>[-0.32621714, -0.66402626, -0.77617854, -0.048...</td>\n",
              "      <td>179444779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.034525</td>\n",
              "      <td>1354.342773</td>\n",
              "      <td>0.168363</td>\n",
              "      <td>2</td>\n",
              "      <td>-51882.875000</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0.037692</td>\n",
              "      <td>[-0.7914866, -0.56841093, -0.90398467, -0.4970...</td>\n",
              "      <td>[-0.36739764, -0.2959777, -0.2437606, -0.16561...</td>\n",
              "      <td>1959</td>\n",
              "      <td>[-0.19288762, -0.09012802, -0.005803698, -0.00...</td>\n",
              "      <td>30631031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13145</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.306524</td>\n",
              "      <td>1356.508667</td>\n",
              "      <td>2.431377</td>\n",
              "      <td>2</td>\n",
              "      <td>-51882.875000</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.076715</td>\n",
              "      <td>[0.25040475, 0.1748516, -0.25876957, 0.4646519...</td>\n",
              "      <td>[0.5349816, 0.10783798, 0.0, 0.062072594, 0.20...</td>\n",
              "      <td>1201</td>\n",
              "      <td>[-0.5099395, -0.2610199, -0.013828868, 0.03111...</td>\n",
              "      <td>160196644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13146</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.018647</td>\n",
              "      <td>1325.826538</td>\n",
              "      <td>0.157572</td>\n",
              "      <td>4</td>\n",
              "      <td>-51882.871094</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.029525</td>\n",
              "      <td>[-0.8488945, -0.76858133, -0.59189904, -0.4129...</td>\n",
              "      <td>[1.103685, 0.92982835, 0.54336816, 0.52445084,...</td>\n",
              "      <td>11523</td>\n",
              "      <td>[-0.1820444, -0.18216795, -0.339928, -0.344022...</td>\n",
              "      <td>270471974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13147</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.086853</td>\n",
              "      <td>1326.775513</td>\n",
              "      <td>0.652589</td>\n",
              "      <td>4</td>\n",
              "      <td>-51882.875000</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.020843</td>\n",
              "      <td>[0.123891085, -0.055945847, 0.07097072, -0.223...</td>\n",
              "      <td>[0.6529525, 0.69058144, 0.80414826, 0.58449507...</td>\n",
              "      <td>8400</td>\n",
              "      <td>[-1.0, -0.57836837, -0.66402966, -0.7568603, -...</td>\n",
              "      <td>141187719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13148</th>\n",
              "      <td>b'PC'</td>\n",
              "      <td>0.162419</td>\n",
              "      <td>1326.658447</td>\n",
              "      <td>2.166031</td>\n",
              "      <td>1</td>\n",
              "      <td>-51882.871094</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.016130</td>\n",
              "      <td>[-0.086693116, -0.07344482, -0.07668034, -0.09...</td>\n",
              "      <td>[0.20690493, 0.08123834, -0.010720634, 0.09892...</td>\n",
              "      <td>5836</td>\n",
              "      <td>[-0.07755127, -0.2760155, 0.1085413, 0.6567023...</td>\n",
              "      <td>349518800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13149</th>\n",
              "      <td>b'J'</td>\n",
              "      <td>0.121973</td>\n",
              "      <td>1332.107910</td>\n",
              "      <td>5.584831</td>\n",
              "      <td>4</td>\n",
              "      <td>-51882.875000</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.051618</td>\n",
              "      <td>[0.7601237, 0.7964004, 0.663667, 0.685883, 0.7...</td>\n",
              "      <td>[0.022403259, -0.06456212, -0.06761711, 0.3498...</td>\n",
              "      <td>11604</td>\n",
              "      <td>[-0.47620365, 0.05174322, -0.7024073, -0.64429...</td>\n",
              "      <td>271693788</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13150 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed103d75-5094-44a5-be2d-4ecb328220ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed103d75-5094-44a5-be2d-4ecb328220ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed103d75-5094-44a5-be2d-4ecb328220ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1942d6c0-d09c-4956-a89f-da318b44c31b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1942d6c0-d09c-4956-a89f-da318b44c31b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1942d6c0-d09c-4956-a89f-da318b44c31b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 13150,\n  \"fields\": [\n    {\n      \"column\": \"Disposition\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"b'J'\",\n          \"b'PC'\",\n          \"b'EB'\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Duration\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 12732,\n        \"samples\": [\n          0.25570806860923767,\n          0.10271184146404266,\n          0.1696682721376419\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Epoc\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 11230,\n        \"samples\": [\n          1329.725341796875,\n          1328.1256103515625,\n          1354.8721923828125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Period\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 10931,\n        \"samples\": [\n          27.070140838623047,\n          0.8163849711418152,\n          0.13461799919605255\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sectors\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          5,\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Transit_Depth\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 123,\n        \"samples\": [\n          -51882.83203125,\n          -51882.59375,\n          -51882.77734375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"camera\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ccd\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"depth_change\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 12797,\n        \"samples\": [\n          -0.5225955843925476,\n          0.18309442698955536,\n          0.9118805527687073\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"global_view\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"local_view\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"row_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4851,\n        \"min\": 0,\n        \"max\": 16487,\n        \"num_unique_values\": 12979,\n        \"samples\": [\n          3837,\n          2698,\n          12199\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"secondary_view\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tic_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 628880440,\n        \"min\": 713184,\n        \"max\": 10005000338,\n        \"num_unique_values\": 12132,\n        \"samples\": [\n          66436093,\n          393948093,\n          410449214\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RnsZYfMeF_A"
      },
      "outputs": [],
      "source": [
        "c = 0\n",
        "train_df = pd.DataFrame(columns=['tic_id', 'global_view', 'local_view', 'secondary_view', 'depth_change', 'Disposition'])\n",
        "\n",
        "for row in train_df['global_view']:\n",
        "    disposition = train_df['Disposition'][c].numpy().decode('utf-8')\n",
        "\n",
        "    data_to_append = pd.DataFrame({\n",
        "        'tic_id': [int(train_df['tic_id'][c].numpy())],\n",
        "        'global_view': [row.numpy().tolist()],\n",
        "        'local_view': [train_df['local_view'][c].numpy().tolist()],\n",
        "        'secondary_view': [train_df['secondary_view'][c].numpy().tolist()],\n",
        "        'depth_change': [float(train_df['depth_change'][c].numpy())],\n",
        "        'Disposition': [disposition]\n",
        "    })\n",
        "\n",
        "    train_df = pd.concat([train_df, data_to_append], ignore_index=True)\n",
        "    c += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A transit depth difference adat kiszámítása"
      ],
      "metadata": {
        "id": "K6-btFjlLPDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Először az outlier értékeket szűröm ki a dolgozatomban említett módon."
      ],
      "metadata": {
        "id": "uvusOYlbLWbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_outliers(df, column, lower_quantile=0.01, upper_quantile=0.99):\n",
        "    \"\"\" Cap and floor the outliers based on quantiles \"\"\"\n",
        "    lower_bound = df[column].quantile(lower_quantile)\n",
        "    upper_bound = df[column].quantile(upper_quantile)\n",
        "    df[column] = np.clip(df[column], lower_bound, upper_bound)\n",
        "    return df\n",
        "\n",
        "train_df = handle_outliers(train_df, 'depth_change')\n",
        "test_df = handle_outliers(test_df, 'depth_change')\n",
        "val_df = handle_outliers(val_df, 'depth_change')\n",
        "\n",
        "# Robust Scaler\n",
        "scaler = RobustScaler()\n",
        "train_df['transit_depth_difference'] = scaler.fit_transform(train_df[['depth_change']])\n",
        "test_df['transit_depth_difference'] = scaler.transform(test_df[['depth_change']])\n",
        "val_df['transit_depth_difference'] = scaler.transform(val_df[['depth_change']])\n",
        "\n",
        "print(\"A normalizált minta a Robust Saler alkalmazása után:\")\n",
        "print(train_df[['depth_change', 'transit_depth_difference']].head())"
      ],
      "metadata": {
        "id": "WcF94vazLOc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-UAYmkzYxd3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "mean_depth_change = train_df['depth_change'].mean()\n",
        "std_depth_change = train_df['depth_change'].std()\n",
        "\n",
        "print(\"Mélységcsökkenés átlaga:\", mean_depth_change)\n",
        "print(\"Mélységcsökkenés szórása:\", std_depth_change)\n",
        "\n",
        "#normalizálom a mélységcsökkenést (depth_change) úgy, hogy kivonom a tanítóhalmaz átlagát, majd elosztom a tanítóhalmaz szórásával\n",
        "def normalize_depth_change(df, mean, std):\n",
        "    if std == 0:\n",
        "        print(\"A szórás értéke 0, így a kapott értékek egyenlőek lesznek.\")\n",
        "    else:\n",
        "        df['transit_depth_difference'] = (df['depth_change'] - mean) / std\n",
        "    return df\n",
        "\n",
        "# normalizálom a három adathalmazt a kapott statisztikai értékek alapján\n",
        "train_df = normalize_depth_change(train_df, mean_depth_change, std_depth_change)\n",
        "test_df = normalize_depth_change(test_df, mean_depth_change, std_depth_change)\n",
        "val_df = normalize_depth_change(val_df, mean_depth_change, std_depth_change)\n",
        "\n",
        "\n",
        "print(\"A mélységcsökkenés értékének terjedelme a tanítóhalmazban:\", train_df['depth_change'].min(), \" - \", train_df['depth_change'].max())\n",
        "print(\"A mélységcsökkenés eloszlása a tanítóhalmazban:\", train_df['depth_change'].var())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beállítom, hogy triage vagy vetting feladatra tanítom a modellt.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l9pXK2_hM1-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mode = 'vetting' # vetting / triage"
      ],
      "metadata": {
        "id": "L1TXJcRaNFNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qzU5IBSjIuZ"
      },
      "outputs": [],
      "source": [
        "if mode == 'vetting':\n",
        "  train_df.loc[train_df['Disposition'] == 'EB', 'Disposition'] = 'J'\n",
        "  test_df.loc[test_df['Disposition'] == 'EB', 'Disposition'] = 'J'\n",
        "  val_df.loc[val_df['Disposition'] == 'EB', 'Disposition'] = 'J'\n",
        "elif mode == 'triage':\n",
        "  train_df.loc[train_df['Disposition'] == 'EB', 'Disposition'] = 'TE'  # TE => \"transit event\", egy közös címke az exobolygójelöltek és a fedési kettőscsillag-rendszerek számára\n",
        "  test_df.loc[test_df['Disposition'] == 'EB', 'Disposition'] = 'TE'\n",
        "  val_df.loc[val_df['Disposition'] == 'EB', 'Disposition'] = 'TE'\n",
        "  train_df.loc[train_df['Disposition'] == 'PC', 'Disposition'] = 'TE'\n",
        "  test_df.loc[test_df['Disposition'] == 'PC', 'Disposition'] = 'TE'\n",
        "  val_df.loc[val_df['Disposition'] == 'PC', 'Disposition'] = 'TE'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A szűrő eljárás a paraméterek és metaparaméterek gyorsabb beállításához."
      ],
      "metadata": {
        "id": "Vu0nANRSz-N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# szűrő eljárás a lokális és másodlagos nézetekhez\n",
        "def filter(flux_array):\n",
        "\n",
        "    classifications = []\n",
        "\n",
        "    first_segment = flux_array[:20]\n",
        "    middle_segment = flux_array[20:41]\n",
        "    third_segment = flux_array[41:]\n",
        "\n",
        "    first_avg = np.mean(first_segment)\n",
        "    middle_avg = np.mean(middle_segment)\n",
        "    third_avg = np.mean(third_segment)\n",
        "\n",
        "    if first_avg < 0.18 and third_avg < 0.18 and -1 < middle_avg < -0.3:\n",
        "        classifications.append('PC')\n",
        "    else:\n",
        "        classifications.append('Not PC')\n",
        "\n",
        "    return np.array(classifications)\n",
        "\n",
        "# szűrő eljárás a globális nézetekhez\n",
        "def filter_global(flux_array):\n",
        "\n",
        "    classifications = []\n",
        "\n",
        "    first_segment = flux_array[:98]\n",
        "    middle_segment = flux_array[98:102]\n",
        "    third_segment = flux_array[102:]\n",
        "\n",
        "    first_avg = np.mean(first_segment)\n",
        "    middle_avg = np.mean(middle_segment)\n",
        "    third_avg = np.mean(third_segment)\n",
        "\n",
        "    if first_avg < 0.05 and third_avg < 0.05 and -1 < middle_avg < -0.2:\n",
        "        classifications.append('PC')\n",
        "    else:\n",
        "        classifications.append('Not PC')\n",
        "\n",
        "    return np.array(classifications)\n",
        "\n",
        "\n",
        "c = 0\n",
        "pc_count = 0\n",
        "eb_count = 0\n",
        "j_count = 0\n",
        "indices = []\n",
        "\n",
        "\n",
        "for i in val_df[\"local_view\"]:\n",
        "  smtgh = filter(i)\n",
        "  if smtgh == \"PC\":\n",
        "    smtgh = filter(val_df[\"secondary_view\"][c])\n",
        "    if smtgh != \"PC\":\n",
        "      smtgh = filter_global(val_df[\"global_view\"][c])\n",
        "      if smtgh == \"PC\":\n",
        "        indices.append(c)\n",
        "        if val_df[\"Disposition\"][c] == \"EB\":\n",
        "          eb_count += 1\n",
        "        if val_df[\"Disposition\"][c] == \"J\":\n",
        "          j_count += 1\n",
        "        if val_df[\"Disposition\"][c] == \"PC\":\n",
        "          pc_count += 1\n",
        "\n",
        "  c += 1\n",
        "\n",
        "print(f\"PC szám: {pc_count}\")\n",
        "print(f\"EB szám: {eb_count}\")\n",
        "print(f\"J szám: {j_count}\")"
      ],
      "metadata": {
        "id": "fdN9fc9bz8wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tanítás"
      ],
      "metadata": {
        "id": "SXyVIR6ONqdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A kiegyensúlyozott batch generálás\n",
        "class BalancedBatchGenerator:\n",
        "    def __init__(self, X, y, batch_size=32, classes=None):\n",
        "        self.X = [X[i] for i in range(len(X))]\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.classes = np.unique(y.argmax(axis=1)) if classes is None else classes\n",
        "        self.class_indices = {cls: np.where(y.argmax(axis=1) == cls)[0] for cls in self.classes}\n",
        "\n",
        "    def generate(self):\n",
        "        while True:\n",
        "            batch_indices = []\n",
        "            per_class = self.batch_size // len(self.classes)\n",
        "\n",
        "            for cls in self.classes:\n",
        "                choices = np.random.choice(self.class_indices[cls], per_class, replace=True)\n",
        "                batch_indices.extend(choices)\n",
        "\n",
        "            if len(batch_indices) < self.batch_size:\n",
        "                remainder = self.batch_size - len(batch_indices)\n",
        "                additional_indices = np.random.choice(np.concatenate(list(self.class_indices.values())), remainder, replace=True)\n",
        "                batch_indices.extend(additional_indices)\n",
        "\n",
        "            np.random.shuffle(batch_indices)\n",
        "            yield [self.X[i][batch_indices] for i in range(len(self.X))], self.y[batch_indices]\n",
        "\n",
        "\n",
        "def tensor_to_value(tensor):\n",
        "    return tensor.numpy().decode('utf-8') if isinstance(tensor, tf.Tensor) else tensor\n",
        "\n",
        "for df in [train_df, test_df, val_df]:\n",
        "    df['Disposition'] = df['Disposition'].apply(tensor_to_value)\n",
        "\n",
        "# Enkódolom a címkéket (One-hot encoding)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(train_df['Disposition'])\n",
        "y_train_encoded = to_categorical(y_train)\n",
        "y_test_encoded = to_categorical(label_encoder.transform(test_df['Disposition']))\n",
        "y_val_encoded = to_categorical(label_encoder.transform(val_df['Disposition']))\n",
        "\n",
        "\n",
        "# Adataugmentációk:\n",
        "def warp_light_curve(light_curve, warp_factor=0.1):\n",
        "    length = len(light_curve)\n",
        "    warp_size = int(warp_factor * length)\n",
        "    start = np.random.randint(0, length - warp_size)\n",
        "    end = start + warp_size\n",
        "\n",
        "    # Kiválasztom az elnyújtani kívánt szegmenst\n",
        "    segment = light_curve[start:end]\n",
        "\n",
        "    # Meghatározom véletlenszerűen a megnyújtás mértékét\n",
        "    if np.random.rand() > 0.5:\n",
        "        # Kompresszió\n",
        "        new_size = np.random.randint(low=warp_size // 2, high=warp_size)\n",
        "    else:\n",
        "        # Nyújtás\n",
        "        new_size = np.random.randint(low=warp_size, high=min(length, warp_size * 2))\n",
        "        if start + new_size > length:\n",
        "            new_size = length - start  # Az új hossz nem haladhatja meg a fénygörbe eredeti hosszát\n",
        "\n",
        "    # Interpoláció\n",
        "    warped_segment = np.interp(\n",
        "        np.linspace(0, warp_size, num=new_size, endpoint=False),\n",
        "        np.arange(warp_size),\n",
        "        segment\n",
        "    )\n",
        "\n",
        "    # Új fénygörbe létrehozása\n",
        "    new_light_curve = np.concatenate([\n",
        "        light_curve[:start],\n",
        "        warped_segment,\n",
        "        light_curve[start + warp_size:]\n",
        "    ])\n",
        "\n",
        "    # Fénygörbe hosszának igazítása\n",
        "    if len(new_light_curve) > length:\n",
        "        new_light_curve = new_light_curve[:length]\n",
        "    elif len(new_light_curve) < length:\n",
        "        new_light_curve = np.pad(new_light_curve, (0, length - len(new_light_curve)), 'constant')\n",
        "\n",
        "    return new_light_curve\n",
        "\n",
        "\n",
        "# Készítettem két másik adataugmentációt is azonban ezekkel egyenlőre nem sikerült jobb eredményeket elérnem, így a kiértékelésnél nem használtam\n",
        "def jitter_light_curve(light_curve, noise_level=0.02):\n",
        "    # Gauss zajjal történő adatagmentáció\n",
        "    noise = np.random.normal(0, noise_level, size=len(light_curve))\n",
        "    return light_curve + noise\n",
        "\n",
        "def randomly_shift_points(series, num_points=61, max_shift=0.2):\n",
        "    # Véletlenszerűen választott adatpontok függőleges irányban történő eltolása. A mérésekkor fellépő zajt szeretném szimulálni.\n",
        "    modified_series = np.copy(series)\n",
        "\n",
        "    indices_to_shift = np.random.choice(len(series), size=num_points, replace=False)\n",
        "\n",
        "    shifts = np.random.uniform(-max_shift, max_shift, size=num_points)\n",
        "\n",
        "    for idx, shift in zip(indices_to_shift, shifts):\n",
        "        modified_series[idx] += shift\n",
        "\n",
        "    return modified_series\n",
        "\n",
        "\n",
        "def augment_light_curves(df, features, warp_factor=0.08):\n",
        "    augmented_rows = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Liang Yu és munkatársai által használt adataugmentáció, vízszintes tükrözés\n",
        "        flip_augmented_row = row.copy()\n",
        "        for feature in features:\n",
        "            if np.random.rand() < 0.5:\n",
        "                flip_augmented_row[feature] = np.flip(row[feature], axis=0).tolist()\n",
        "        augmented_rows.append(flip_augmented_row)\n",
        "\n",
        "        # A dolgozatomban emltített másik adataugmentációm, a szeletek nyújtása\n",
        "        warp_augmented_row = row.copy()\n",
        "        for feature in features:\n",
        "          if np.random.rand() < 0.5:\n",
        "            light_curve = np.array(row[feature])\n",
        "            warped_light_curve = warp_light_curve(light_curve, warp_factor=warp_factor)\n",
        "            warp_augmented_row[feature] = warped_light_curve.tolist()\n",
        "        augmented_rows.append(warp_augmented_row)\n",
        "\n",
        "\n",
        "    augmented_df = pd.DataFrame(augmented_rows)\n",
        "    return pd.concat([df, augmented_df], ignore_index=True)\n",
        "\n",
        "# A globális, lokális és másodlagos nézeteket is augmentálom\n",
        "features_to_augment = ['global_view', 'local_view', 'secondary_view']\n",
        "\n",
        "# Adataugmentáció alkalmazása a tanítóhalmazra\n",
        "augmented_train_df = augment_light_curves(train_df, features_to_augment)\n",
        "augmented_train_df = augmented_train_df.sample(frac=1).reset_index(drop=True)\n",
        "y_train_encoded = to_categorical(label_encoder.transform(augmented_train_df['Disposition']))\n",
        "\n",
        "\n",
        "# Input adatok előkészítése\n",
        "def prepare_data(df):\n",
        "    X_global = np.array(df['global_view'].tolist()).reshape((-1, 201, 1))\n",
        "    X_local = np.array(df['local_view'].tolist()).reshape((-1, 61, 1))\n",
        "    X_secondary = np.array(df['secondary_view'].tolist()).reshape((-1, 61, 1))\n",
        "    X_depth_change = np.array((df['transit_depth_difference'] * 4).tolist()).reshape((-1, 1)) # a dolgozatban említett módon a \"transit_depth_difference\" értéket néggyel szorzom\n",
        "    return X_global, X_local, X_secondary, X_depth_change\n",
        "\n",
        "X_global_train, X_local_train, X_secondary_train, X_depth_change_train = prepare_data(augmented_train_df)\n",
        "X_global_test, X_local_test, X_secondary_test, X_depth_change_test = prepare_data(test_df)\n",
        "X_global_val, X_local_val, X_secondary_val, X_depth_change_val = prepare_data(val_df)\n",
        "\n",
        "\n",
        "# Konvolúciós neurális hálózat\n",
        "\n",
        "# Az első CNN ágam, amelyet a lokális és másodlagos nézeteknél használok\n",
        "def create_cnn_branch(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    # Első konvolúciós réteg\n",
        "    x = Conv1D(16, 5, padding='same', kernel_regularizer=l2(0.001))(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    # Második konvolúciós réteg\n",
        "    x = Conv1D(32, 5, padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    # Harmadik konvolúciós réteg\n",
        "    x = Conv1D(64, 5, padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Flatten()(x)\n",
        "    return input_layer, x\n",
        "\n",
        "# A másik, mélyebb ágam a globális nézethez\n",
        "def create_cnn_branch_deep(input_shape):\n",
        "    input_layer = Input(shape=input_shape)\n",
        "    # Első konvolúciós réteg\n",
        "    x = Conv1D(32, 5, padding='same', kernel_regularizer=l2(0.001))(input_layer)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    # Második konvolúciós réteg\n",
        "    x = Conv1D(128, 5, padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    # Harmadik konvolúciós réteg\n",
        "    x = Conv1D(128, 5, padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "    # Negyedik konvolúciós réteg\n",
        "    x = Conv1D(256, 5, padding='same', kernel_regularizer=l2(0.001))(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling1D(2)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Flatten()(x)\n",
        "    return input_layer, x\n",
        "\n",
        "# A tanulási ráta szabályozására használt egyszerű scheduler\n",
        "def scheduler(epoch, lr):\n",
        "  if epoch < 2:\n",
        "      return lr\n",
        "  else:\n",
        "      return lr * tf.math.exp(-0.1)\n",
        "\n",
        "\n",
        "local_input, local_features = create_cnn_branch((61, 1))\n",
        "global_input, global_features = create_cnn_branch_deep((201, 1))\n",
        "secondary_input, secondary_features = create_cnn_branch((61, 1))\n",
        "\n",
        "depth_change_input = Input(shape=(1,))\n",
        "\n",
        "# Egyesítem a jellemzőket, itt kerül elő a \"transit_depth_difference\" is\n",
        "combined_features = concatenate([local_features, global_features, secondary_features, depth_change_input])\n",
        "\n",
        "# Teljesen összekötött rétegek\n",
        "x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(combined_features)\n",
        "x = Dropout(0.4)(x)\n",
        "x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(y_train_encoded.shape[1], activation='softmax')(x)\n",
        "\n",
        "\n",
        "# A modell\n",
        "model = Model(inputs=[local_input, global_input, secondary_input, depth_change_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Korai megállás\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1),\n",
        "    LearningRateScheduler(scheduler, verbose=1)\n",
        "]\n",
        "\n",
        "# A kiegyensúlyozatlanság miatt bevezetett súlyok, azonban végül nem segítettek a tanítás során így nem használtam. Fenntartom a lehetőségét,\n",
        "# hogy további finomhangolások után segíthetnek a modell eredményességében, így egyenlőre megtartottam a kódrészletet.\n",
        "y_train_labels = np.argmax(y_train_encoded, axis=1)\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_labels), y=y_train_labels)\n",
        "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "# Kiegyensúlyozott batch generálás\n",
        "batch_size = 75\n",
        "generator = BalancedBatchGenerator([X_local_train, X_global_train, X_secondary_train, X_depth_change_train], y_train_encoded, batch_size=batch_size)\n",
        "validation_data = ([X_local_val, X_global_val, X_secondary_val, X_depth_change_val], y_val_encoded)\n",
        "\n",
        "# Modell tanítása\n",
        "history = model.fit(\n",
        "    generator.generate(),\n",
        "    steps_per_epoch=len(y_train_encoded) // batch_size,\n",
        "    validation_data=validation_data,\n",
        "    epochs=30,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "predictions = model.predict([X_local_test, X_global_test, X_secondary_test, X_depth_change_test])\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "# Modell kiértékelése\n",
        "results = model.evaluate([X_local_test, X_global_test, X_secondary_test, X_depth_change_test], y_test_encoded)\n",
        "print(f'Test Accuracy: {results[1]*100:.2f}%')\n",
        "\n",
        "report = classification_report(true_classes, predicted_classes, target_names=label_encoder.classes_)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "9Dm-zZXM1zZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision-Recall görbe"
      ],
      "metadata": {
        "id": "KratLpoxN_Zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Egy modell kiértékeléséhez"
      ],
      "metadata": {
        "id": "g-ngjjWMOBru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.argmax(y_test_encoded, axis=1)\n",
        "class_of_interest = 1  # itt állíthatjuk be, hogy melyik osztályra szeretnénk kirajzolni a precision-recall görbét (jelen esetben a 0 a J osztályt, az 1 pedig a PC osztályt reprezentálja)\n",
        "y_test_binary = (y_test == class_of_interest).astype(int)\n",
        "\n",
        "# Valószínűségek összegyűjtése.\n",
        "probabilities = model.predict([X_local_test, X_global_test, X_secondary_test, X_depth_change_test])\n",
        "class_probabilities = probabilities[:, class_of_interest]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test_binary, class_probabilities)\n",
        "\n",
        "# Görbe kirajzolása.\n",
        "plt.figure(figsize=(7, 6))\n",
        "plt.title('Precision-Recall görbe - PC osztály'.format(label_encoder.classes_[class_of_interest]))\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GOakdy-_z3P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Több modell átlagolt kiértékeléséhez"
      ],
      "metadata": {
        "id": "p3BTj81_OE7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_models(model_files, X_inputs):\n",
        "    predictions = []\n",
        "    for model_file in model_files:\n",
        "        model = load_model(model_file)\n",
        "        predictions.append(model.predict(X_inputs))\n",
        "\n",
        "    # Átlagoljuk a predikciókat.\n",
        "    predictions = np.array(predictions)\n",
        "    mean_predictions = np.mean(predictions, axis=0)\n",
        "    return mean_predictions\n",
        "\n",
        "def plot_precision_recall_curve(y_test_encoded, mean_predictions, class_of_interest, class_labels):\n",
        "    y_test = np.argmax(y_test_encoded, axis=1)\n",
        "    y_test_binary = (y_test == class_of_interest).astype(int)\n",
        "\n",
        "    class_probabilities = mean_predictions[:, class_of_interest]\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test_binary, class_probabilities)\n",
        "\n",
        "    # Precision-recall görbe kirajolása\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    plt.title('Precision-Recall görbe - PC osztály'.format(class_labels[class_of_interest]))\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# A modellek fájlneveinek listája\n",
        "model_files = [f'model_ogval_{i}.h5' for i in range(1, 8)]\n",
        "\n",
        "mean_predictions = predict_with_models(model_files, [X_local_test, X_global_test, X_secondary_test, X_depth_change_test])\n",
        "plot_precision_recall_curve(y_test_encoded, mean_predictions, 1, label_encoder.classes_)"
      ],
      "metadata": {
        "id": "D2Gm1d6zOHIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# modell mentése\n",
        "model.save('model_1.h5')"
      ],
      "metadata": {
        "id": "jSYvmrjuKxQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2POcRbfbSc0"
      },
      "outputs": [],
      "source": [
        "model_path = 'model_1.h5'\n",
        "# Korábbi modell betöltése\n",
        "model = load_model(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keresztvalidáció"
      ],
      "metadata": {
        "id": "wG5cKP7ZOyEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BalancedBatchGenerator:\n",
        "    def __init__(self, X, y, batch_size=32, classes=None):\n",
        "        self.X = [X[i] for i in range(len(X))]\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.classes = np.unique(y.argmax(axis=1)) if classes is None else classes\n",
        "        self.class_indices = {cls: np.where(y.argmax(axis=1) == cls)[0] for cls in self.classes}\n",
        "\n",
        "    def generate(self):\n",
        "        while True:\n",
        "            batch_indices = []\n",
        "            per_class = self.batch_size // len(self.classes)\n",
        "\n",
        "            for cls in self.classes:\n",
        "                choices = np.random.choice(self.class_indices[cls], per_class, replace=True)\n",
        "                batch_indices.extend(choices)\n",
        "\n",
        "            if len(batch_indices) < self.batch_size:\n",
        "                remainder = self.batch_size - len(batch_indices)\n",
        "                additional_indices = np.random.choice(np.concatenate(list(self.class_indices.values())), remainder, replace=True)\n",
        "                batch_indices.extend(additional_indices)\n",
        "\n",
        "            np.random.shuffle(batch_indices)\n",
        "            yield [self.X[i][batch_indices] for i in range(len(self.X))], self.y[batch_indices]\n",
        "\n",
        "# Tanító- és validációshalmaz egyesítése\n",
        "all_train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "\n",
        "# Címkénk enkódolása (one-hot)\n",
        "label_encoder = LabelEncoder()\n",
        "all_labels = label_encoder.fit_transform(all_train_df['Disposition'])\n",
        "all_labels_encoded = to_categorical(all_labels)\n",
        "\n",
        "def create_model():\n",
        "    local_input, local_features = create_cnn_branch((61, 1))\n",
        "    global_input, global_features = create_cnn_branch_deep((201, 1))\n",
        "    secondary_input, secondary_features = create_cnn_branch((61, 1))\n",
        "    depth_change_input = Input(shape=(1,))\n",
        "\n",
        "    combined_features = concatenate([local_features, global_features, secondary_features, depth_change_input])\n",
        "\n",
        "    x = Dense(256, activation='relu', kernel_regularizer=l2(0.001))(combined_features)\n",
        "    x = Dropout(0.4)(x)\n",
        "    x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(Y_all.shape[1], activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=[local_input, global_input, secondary_input, depth_change_input], outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def augment_light_curves(df, features, warp_factor=0.08):\n",
        "    augmented_rows = []\n",
        "    for _, row in df.iterrows():\n",
        "        flip_augmented_row = row.copy()\n",
        "        for feature in features:\n",
        "            if np.random.rand() < 0.5:\n",
        "                flip_augmented_row[feature] = np.flip(row[feature], axis=0).tolist()\n",
        "        augmented_rows.append(flip_augmented_row)\n",
        "\n",
        "        warp_augmented_row = row.copy()\n",
        "        for feature in features:\n",
        "          if np.random.rand() < 0.5:\n",
        "            light_curve = np.array(row[feature])\n",
        "            warped_light_curve = warp_light_curve(light_curve, warp_factor=warp_factor)\n",
        "            warp_augmented_row[feature] = warped_light_curve.tolist()\n",
        "        augmented_rows.append(warp_augmented_row)\n",
        "\n",
        "    augmented_df = pd.DataFrame(augmented_rows)\n",
        "    return pd.concat([df, augmented_df], ignore_index=True)\n",
        "\n",
        "\n",
        "def prepare_data(df):\n",
        "    X_global = np.array(df['global_view'].tolist()).reshape((-1, 201, 1))\n",
        "    X_local = np.array(df['local_view'].tolist()).reshape((-1, 61, 1))\n",
        "    X_secondary = np.array(df['secondary_view'].tolist()).reshape((-1, 61, 1))\n",
        "    X_depth_change = np.array((df['transit_depth_difference'] * 4).tolist()).reshape((-1, 1))\n",
        "    return [X_local, X_global, X_secondary, X_depth_change]\n",
        "\n",
        "features_to_augment = ['global_view', 'local_view', 'secondary_view']\n",
        "augmented_train_df = augment_light_curves(all_train_df, features_to_augment)\n",
        "X_all = prepare_data(augmented_train_df)\n",
        "Y_all = to_categorical(label_encoder.transform(augmented_train_df['Disposition']))\n",
        "\n",
        "X_test = prepare_data(test_df)\n",
        "Y_test = to_categorical(label_encoder.transform(test_df['Disposition']))\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "for fold_no, (train_index, val_index) in enumerate(kf.split(X_all[0])):\n",
        "    X_train, X_val = [x[train_index] for x in X_all], [x[val_index] for x in X_all]\n",
        "    y_train, y_val = Y_all[train_index], Y_all[val_index]\n",
        "\n",
        "    assert max(train_index) < len(Y_all), \"Train index out of bounds\"\n",
        "    assert max(val_index) < len(Y_all), \"Validation index out of bounds\"\n",
        "\n",
        "    callbacks = [\n",
        "    EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True),\n",
        "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1),\n",
        "    LearningRateScheduler(scheduler, verbose=1)\n",
        "    ]\n",
        "\n",
        "    model = create_model()\n",
        "    print(f'Training fold {fold_no + 1}...')\n",
        "    batch_size = 75\n",
        "    generator = BalancedBatchGenerator([X_local_train, X_global_train, X_secondary_train, X_depth_change_train], y_train_encoded, batch_size=batch_size)\n",
        "    validation_data = ([X_local_val, X_global_val, X_secondary_val, X_depth_change_val], y_val_encoded)\n",
        "\n",
        "    history = model.fit(\n",
        "        generator.generate(),\n",
        "        steps_per_epoch=len(y_train_encoded) // batch_size,\n",
        "        validation_data=validation_data,\n",
        "        epochs=30,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # Mentsük el a modellt\n",
        "    model.save(f'model_fold_{fold_no+1}.h5')\n",
        "\n",
        "\n",
        "results = model.evaluate(X_test, Y_test)\n",
        "print(f'Test Accuracy: {results[1]*100:.2f}%')\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(Y_test, axis=1)\n",
        "report = classification_report(true_classes, predicted_classes, target_names=label_encoder.classes_)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "8eGKXoG_O3d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A dolgozat 8.1.-es pontjában említett tanítóhalmaz és teszthalmaz közötti átfedés."
      ],
      "metadata": {
        "id": "Q_T1KlF5znhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['local_view_tuple'] = train_df['local_view'].apply(tuple)\n",
        "test_df['local_view_tuple'] = test_df['local_view'].apply(tuple)\n",
        "\n",
        "train_views_set = set(train_df['local_view_tuple'])\n",
        "\n",
        "count = test_df['local_view_tuple'].apply(lambda x: x in train_views_set).sum()\n",
        "\n",
        "print(f\"Átfedések száma: {count}\")"
      ],
      "metadata": {
        "id": "xP-pWhBFzvGd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}